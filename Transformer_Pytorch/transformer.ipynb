{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
    "\n",
    "# Algorith Summery\n",
    "\n",
    "## Input Processing:\n",
    "\n",
    "1. Source and target sequences are converted to embeddings\n",
    "2. Positional encodings are added to provide position information\n",
    "3. Masks are generated to handle padding and prevent looking at future tokens\n",
    "\n",
    "\n",
    "## Encoder Operation:\n",
    "\n",
    "Input goes through multiple identical encoder layers. Each encoder layer has two sub-layers:\n",
    "1. Multi-head self-attention\n",
    "2. Position-wise feed-forward network\n",
    "3. Layer normalization and residual connections are used after each sub-layer\n",
    "\n",
    "\n",
    "## Multi-Head Attention Mechanism:\n",
    "\n",
    "Divides input into multiple heads (to process in parallel). For each head:\n",
    "1. Transform input into Query (Q), Key (K), and Value (V) matrices\n",
    "2. Compute attention scores: (Q × K^T) / √d_k\n",
    "3. Apply softmax to get attention weights\n",
    "4. Multiply weights with Values (V)\n",
    "5. Combine heads and apply final transformation\n",
    "\n",
    "\n",
    "## Decoder Operation:\n",
    "\n",
    "Output sequence goes through multiple identical decoder layers.\n",
    "Each decoder layer has three sub-layers:\n",
    "1. Masked multi-head self-attention (prevents attending to future tokens)\n",
    "2. Multi-head cross-attention (attends to encoder output)\n",
    "3. Position-wise feed-forward network\n",
    "4. Layer normalization and residual connections after each sub-layer\n",
    "\n",
    "\n",
    "## Final Output Generation:\n",
    "\n",
    "The decoder output is projected to the target vocabulary size\n",
    "Can be used to predict the next token in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*--TCGWYxwASbv2ra.png\" width=\"500\"/>\n",
    "\n",
    "The Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence. It consists of multiple “attention heads” that capture different aspects of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Initialize multi-head attention module\n",
    "        Args:\n",
    "            d_model: dimension of model (embedding dimension)\n",
    "            num_heads: number of attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model  # Model's dimension\n",
    "        self.num_heads = num_heads  # Number of parallel attention heads\n",
    "        self.d_k = d_model // num_heads  # Dimension of each head's key/query/value\n",
    "        \n",
    "        # Linear transformations for Query, Key, Value, and Output\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Compute scaled dot-product attention\n",
    "        Args:\n",
    "            Q: Query matrix\n",
    "            K: Key matrix\n",
    "            V: Value matrix\n",
    "            mask: Optional mask to prevent attention to certain positions\n",
    "        \"\"\"\n",
    "        # Compute attention scores (Q × K^T)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        # Scale scores by sqrt(d_k) to prevent extremely small gradients\n",
    "        attn_scores = attn_scores / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (for padding or future tokens)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Compute weighted sum of values\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k)\"\"\"\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine the split heads back together\"\"\"\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention\n",
    "        Args:\n",
    "            Q: Query input\n",
    "            K: Key input\n",
    "            V: Value input\n",
    "            mask: Optional attention mask\n",
    "        \"\"\"\n",
    "        # Transform and split heads for Query, Key, and Value\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Compute attention output\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise Feed-Forward Networks\n",
    "This network enables the model to consider the position of input elements while making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        \"\"\"\n",
    "        Initialize feed-forward network\n",
    "        Args:\n",
    "            d_model: Input/output dimension\n",
    "            d_ff: Hidden layer dimension\n",
    "        \"\"\"\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)  # First linear transformation\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)  # Second linear transformation\n",
    "        self.relu = nn.ReLU()  # ReLU activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply two linear transformations with ReLU activation\"\"\"\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "Positional Encoding is used to inject the position information of each token in the input sequence.\n",
    "It uses sine and cosine functions of different frequencies to generate the positional encoding.\n",
    "The forward method computes the positional encoding by adding the stored positional encoding values to the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        \"\"\"\n",
    "        Initialize positional encoding\n",
    "        Args:\n",
    "            d_model: Embedding dimension\n",
    "            max_seq_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (won't be updated during training)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings\"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Layer\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:552/format:webp/0*bPKV4ekQr9ZjYkWJ.png\" hight=500>\n",
    "\n",
    "An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward layer, and two Layer Normalization layers.\n",
    "\n",
    "The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result.\n",
    "Then, it computes the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        Initialize encoder layer\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward network dimension\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # First layer normalization\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # Second layer normalization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass of encoder layer\n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            mask: Attention mask\n",
    "        \"\"\"\n",
    "        # Self attention block\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))  # Add & Norm\n",
    "        \n",
    "        # Feed forward block\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))  # Add & Norm\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Layer\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:552/format:webp/0*SPZgT4k8GQi37H__.png\" hight=500>\n",
    "\n",
    "A Decoder layer consists of two Multi-Head Attention layers, a Position-wise Feed-Forward layer, and three Layer Normalization layers.\n",
    "\n",
    "The forward method computes the decoder layer output by performing the following steps:\n",
    "\n",
    "1. Calculate the masked self-attention output and add it to the input tensor, followed by dropout and layer normalization.\n",
    "2. Compute the cross-attention output between the decoder and encoder outputs, and add it to the normalized masked self-attention output, followed by dropout and layer normalization.\n",
    "3. Calculate the position-wise feed-forward output and combine it with the normalized cross-attention output, followed by dropout and layer normalization.\n",
    "4. Return the processed tensor.\n",
    "\n",
    "These operations enable the decoder to generate target sequences based on the input and the encoder output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        Initialize decoder layer\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward network dimension\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of decoder layer\n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            enc_output: Encoder output\n",
    "            src_mask: Source sequence mask\n",
    "            tgt_mask: Target sequence mask\n",
    "        \"\"\"\n",
    "        # Self attention block\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Cross attention block\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed forward block\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model (Encoder + Decoder)\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*ljYs7oOlKC71SzSr.png\" width=500>\n",
    "\n",
    "The generate_mask method creates binary masks for source and target sequences to ignore padding tokens and prevent the decoder from attending to future tokens. The forward method computes the Transformer model’s output through the following steps:\n",
    "\n",
    "1. Generate source and target masks using the generate_mask method.\n",
    "2. Compute source and target embeddings, and apply positional encoding and dropout.\n",
    "3. Process the source sequence through encoder layers, updating the enc_output tensor.\n",
    "4. Process the target sequence through decoder layers, using enc_output and masks, and updating the dec_output tensor.\n",
    "5. Apply the linear projection layer to the decoder output, obtaining output logits.\n",
    "\n",
    "\n",
    "These steps enable the Transformer model to process input sequences and generate output sequences based on the combined functionality of its components.\n",
    "\n",
    "\n",
    "## How is a full Transformer (encoder + decoder) different than a decoder-only architecture?\n",
    "\n",
    "The main differences are:\n",
    "\n",
    "1. Information Access Pattern\n",
    "- Full Transformer: The encoder processes the entire input sequence in parallel, creating a rich contextual representation that the decoder can access at every step. The decoder can attend to any encoded input state at any time through cross-attention.\n",
    "- Decoder-only: Only has access to previous tokens in the sequence (through causal/masked self-attention). It must process information sequentially and can't look at future tokens.\n",
    "\n",
    "1. Architecture Components\n",
    "- Full Transformer:\n",
    "  - Encoder: Has self-attention layers that can see all input tokens\n",
    "  - Decoder: Has both masked self-attention (for target sequence) and cross-attention (to access encoder representations)\n",
    "- Decoder-only: Only has masked self-attention layers\n",
    "\n",
    "1. Typical Use Cases\n",
    "- Full Transformer: Best for tasks requiring complex input understanding and transformation\n",
    "  - Machine translation (input: source language, output: target language)\n",
    "  - Summarization (input: long text, output: summary)\n",
    "- Decoder-only: Better for generative tasks and completing sequences\n",
    "  - Language modeling\n",
    "  - Text generation\n",
    "  - Code completion\n",
    "\n",
    "1. Memory & Computation\n",
    "- Full Transformer: \n",
    "  - Processes input once through encoder\n",
    "  - Encoder representations are cached and reused by decoder\n",
    "  - Generally more computationally intensive\n",
    "- Decoder-only:\n",
    "  - Simpler architecture but needs to encode all information in the same sequence\n",
    "  - Can be more memory-efficient but might need longer sequences\n",
    "\n",
    "Here's a simplified visualization using LaTeX:\n",
    "\n",
    "For Full Transformer:\n",
    "$$\n",
    "\\text{Input} \\xrightarrow{\\text{Encoder}} \\text{Hidden States} \\xrightarrow{\\text{Cross-Attention}} \\text{Decoder} \\xrightarrow{} \\text{Output}\n",
    "$$\n",
    "\n",
    "For Decoder-only:\n",
    "$$\n",
    "\\text{Input} \\xrightarrow{\\text{Masked Self-Attention}} \\text{Output}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        \"\"\"\n",
    "        Initialize transformer model\n",
    "        Args:\n",
    "            src_vocab_size: Source vocabulary size\n",
    "            tgt_vocab_size: Target vocabulary size\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            num_layers: Number of encoder/decoder layers\n",
    "            d_ff: Feed-forward network dimension\n",
    "            max_seq_length: Maximum sequence length\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # Create encoder and decoder layers\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        # Output projection\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Generate masks for source and target sequences\n",
    "        Args:\n",
    "            src: Source sequence\n",
    "            tgt: Target sequence\n",
    "        \"\"\"\n",
    "        # Create mask for source sequence (1 for tokens, 0 for padding)\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        # Create mask for target sequence\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        \n",
    "        # Create mask to prevent attention to future tokens\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        \n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Forward pass of transformer\n",
    "        Args:\n",
    "            src: Source sequence\n",
    "            tgt: Target sequence\n",
    "        \"\"\"\n",
    "        # Generate masks\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        \n",
    "        # Embed and apply positional encoding to source sequence\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        \n",
    "        # Embed and apply positional encoding to target sequence\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        # Pass through encoder layers\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        # Pass through decoder layers\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        # Project to vocabulary size\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.685790061950684\n",
      "Epoch: 2, Loss: 8.556265830993652\n",
      "Epoch: 3, Loss: 8.483915328979492\n",
      "Epoch: 4, Loss: 8.432638168334961\n",
      "Epoch: 5, Loss: 8.376571655273438\n",
      "Epoch: 6, Loss: 8.31143856048584\n",
      "Epoch: 7, Loss: 8.231369018554688\n",
      "Epoch: 8, Loss: 8.148785591125488\n",
      "Epoch: 9, Loss: 8.068408012390137\n",
      "Epoch: 10, Loss: 7.988169193267822\n",
      "Epoch: 11, Loss: 7.907508850097656\n",
      "Epoch: 12, Loss: 7.820796966552734\n",
      "Epoch: 13, Loss: 7.740450382232666\n",
      "Epoch: 14, Loss: 7.650052547454834\n",
      "Epoch: 15, Loss: 7.574429035186768\n",
      "Epoch: 16, Loss: 7.4861321449279785\n",
      "Epoch: 17, Loss: 7.406088352203369\n",
      "Epoch: 18, Loss: 7.326938629150391\n",
      "Epoch: 19, Loss: 7.251533508300781\n",
      "Epoch: 20, Loss: 7.166291236877441\n",
      "Epoch: 21, Loss: 7.081549644470215\n",
      "Epoch: 22, Loss: 6.999995231628418\n",
      "Epoch: 23, Loss: 6.930159091949463\n",
      "Epoch: 24, Loss: 6.8478007316589355\n",
      "Epoch: 25, Loss: 6.7815070152282715\n",
      "Epoch: 26, Loss: 6.703404426574707\n",
      "Epoch: 27, Loss: 6.626461029052734\n",
      "Epoch: 28, Loss: 6.5552897453308105\n",
      "Epoch: 29, Loss: 6.4825544357299805\n",
      "Epoch: 30, Loss: 6.418124198913574\n",
      "Epoch: 31, Loss: 6.345730304718018\n",
      "Epoch: 32, Loss: 6.278888702392578\n",
      "Epoch: 33, Loss: 6.197280406951904\n",
      "Epoch: 34, Loss: 6.136287689208984\n",
      "Epoch: 35, Loss: 6.068896770477295\n",
      "Epoch: 36, Loss: 6.002921104431152\n",
      "Epoch: 37, Loss: 5.935272693634033\n",
      "Epoch: 38, Loss: 5.869253635406494\n",
      "Epoch: 39, Loss: 5.81276798248291\n",
      "Epoch: 40, Loss: 5.753123760223389\n",
      "Epoch: 41, Loss: 5.694666862487793\n",
      "Epoch: 42, Loss: 5.630402088165283\n",
      "Epoch: 43, Loss: 5.565728664398193\n",
      "Epoch: 44, Loss: 5.50947380065918\n",
      "Epoch: 45, Loss: 5.440611362457275\n",
      "Epoch: 46, Loss: 5.386929035186768\n",
      "Epoch: 47, Loss: 5.317804336547852\n",
      "Epoch: 48, Loss: 5.2662129402160645\n",
      "Epoch: 49, Loss: 5.213816165924072\n",
      "Epoch: 50, Loss: 5.155918121337891\n",
      "Epoch: 51, Loss: 5.09240198135376\n",
      "Epoch: 52, Loss: 5.035065174102783\n",
      "Epoch: 53, Loss: 4.9854912757873535\n",
      "Epoch: 54, Loss: 4.923969745635986\n",
      "Epoch: 55, Loss: 4.873918056488037\n",
      "Epoch: 56, Loss: 4.818984031677246\n",
      "Epoch: 57, Loss: 4.771726131439209\n",
      "Epoch: 58, Loss: 4.709470748901367\n",
      "Epoch: 59, Loss: 4.655123710632324\n",
      "Epoch: 60, Loss: 4.612065315246582\n",
      "Epoch: 61, Loss: 4.550290584564209\n",
      "Epoch: 62, Loss: 4.494741439819336\n",
      "Epoch: 63, Loss: 4.448899269104004\n",
      "Epoch: 64, Loss: 4.400906562805176\n",
      "Epoch: 65, Loss: 4.354245662689209\n",
      "Epoch: 66, Loss: 4.294393062591553\n",
      "Epoch: 67, Loss: 4.245839595794678\n",
      "Epoch: 68, Loss: 4.201471328735352\n",
      "Epoch: 69, Loss: 4.140115261077881\n",
      "Epoch: 70, Loss: 4.095566749572754\n",
      "Epoch: 71, Loss: 4.053431987762451\n",
      "Epoch: 72, Loss: 3.9981186389923096\n",
      "Epoch: 73, Loss: 3.9469666481018066\n",
      "Epoch: 74, Loss: 3.9020068645477295\n",
      "Epoch: 75, Loss: 3.8548152446746826\n",
      "Epoch: 76, Loss: 3.809636354446411\n",
      "Epoch: 77, Loss: 3.7633423805236816\n",
      "Epoch: 78, Loss: 3.7290725708007812\n",
      "Epoch: 79, Loss: 3.671804428100586\n",
      "Epoch: 80, Loss: 3.6229770183563232\n",
      "Epoch: 81, Loss: 3.567775249481201\n",
      "Epoch: 82, Loss: 3.525787353515625\n",
      "Epoch: 83, Loss: 3.4830753803253174\n",
      "Epoch: 84, Loss: 3.433197021484375\n",
      "Epoch: 85, Loss: 3.3900537490844727\n",
      "Epoch: 86, Loss: 3.338202476501465\n",
      "Epoch: 87, Loss: 3.2975587844848633\n",
      "Epoch: 88, Loss: 3.252389907836914\n",
      "Epoch: 89, Loss: 3.2104692459106445\n",
      "Epoch: 90, Loss: 3.1680996417999268\n",
      "Epoch: 91, Loss: 3.118150234222412\n",
      "Epoch: 92, Loss: 3.075725555419922\n",
      "Epoch: 93, Loss: 3.034144163131714\n",
      "Epoch: 94, Loss: 2.9899747371673584\n",
      "Epoch: 95, Loss: 2.9556820392608643\n",
      "Epoch: 96, Loss: 2.9044554233551025\n",
      "Epoch: 97, Loss: 2.8636114597320557\n",
      "Epoch: 98, Loss: 2.8208158016204834\n",
      "Epoch: 99, Loss: 2.779600143432617\n",
      "Epoch: 100, Loss: 2.735975980758667\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
